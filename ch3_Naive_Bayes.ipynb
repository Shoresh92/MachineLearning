{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Naive Bayes\n",
    "\n",
    "Among the lecture notes, videos and artcles I read, I found the [introduction to Naive Bayes](http://scikit-learn.org/stable/modules/naive_bayes.html) on scikit-learn website the most useful. to the page. For more extensive discussions, I suggest other books like [Baysian Data Analytics](http://www.amazon.com/Bayesian-Analysis-Chapman-Statistical-Science/dp/1439840954/) recommneded in a post on [Quora](https://www.quora.com/What-are-some-good-books-for-learning-probability-and-statistics).\n",
    "\n",
    "To understand it better, and in accordance to what suggested [here](http://machinelearningmastery.com/how-to-learn-a-machine-learning-algorithm/), I will try to answer some questions aiming at gaining a better grasp of the fundamental ideas behing Naive Bayes. \n",
    "\n",
    "### ** What are the fundamental concepts I need to know in order to better understand Naive Bayes? **\n",
    "\n",
    "* ** maximum a posteriori probability (MAP) ** \n",
    "\n",
    "In Bayesian statistics, a maximum a posteriori probability (MAP) estimate is a mode of the posterior distribution. The MAP can be used to obtain a point estimate of an unobserved quantity on the basis of empirical data. \n",
    "\n",
    "* ** Maximum Likelihood **\n",
    "\n",
    "In general, for a fixed set of data and underlying statistical model, the method of maximum likelihood selects the set of values of the model parameters that maximizes the likelihood function. Intuitively, this maximizes the \"agreement\" of the selected model with the observed data, and for discrete random variables it indeed maximizes *the probability of the observed data* under the resulting distribution. [wiki](https://en.wikipedia.org/wiki/Maximum_likelihood) *I am curious to learn more about the mathematical foundation behind it.* \n",
    "\n",
    "* ** posterier probability **\n",
    "\n",
    "The posterior probability is the probability of the parameters Y given the evidence X: p(Y|X).\n",
    "\n",
    "* **Maximum A Posteriori (MAP) estimation**\n",
    "[Source](https://www.cs.utah.edu/~suyash/Dissertation_html/node8.html)\n",
    "\n",
    "Sometimes we have a priori information about the physical process whose parameters we want to estimate. Such information can come either from the correct scientific knowledge of the physical process or from previous empirical evidence. We can encode such prior information in terms of a probability density function (PDF) on the parameter to be estimated. Essentially, we treat the parameter Y as the value of an random variable (RV). The associated probabilities P(Y) are called the prior probabilities. We refer to the inference based on such priors as **Bayesian inference**. Bayes' theorem shows [...] way for incorporating prior information in the estimation process: \n",
    "\n",
    "$$ P(Y|X) = P(X|Y)\\frac{P(Y)}{P(X)} $$\n",
    "\n",
    "The term on the left hand side is called the **posterior**. On the right hand side, the numerator is the product of the **likelihood term** and the **prior term**. The denominator serves as a **normalization term** so that the posterior PDF integrates to unity. Thus, Bayesian inference produces the maximum a posteriori (MAP) estimate \n",
    "$argmax_Y P(Y|X)= argmax_Y P(X|Y) P(Y)$\n",
    "\n",
    "*Also, to better understand the definition and concepts, reading [this link](http://www.probabilitycourse.com/chapter9/9_1_2_MAP_estimation.php) is highly recommended.*\n",
    "\n",
    "* **Point estimator**\n",
    "\n",
    "In statistics, point estimation involves the use of sample data to calculate a single value (known as a statistic) which is to serve as a \"best guess\" or \"best estimate\" of an unknown (fixed or random) population parameter.\n",
    "\n",
    "* **Regularization**: \n",
    "\n",
    "in mathematics and statistics and particularly in the fields of machine learning and inverse problems, refers to a process of introducing additional information in order to solve an ill-posed problem or to prevent overfitting. [Wiki](https://en.wikipedia.org/wiki/Regularization_(mathematics)\n",
    "\n",
    "In spite of their apparently over-simplified assumptions, naive Bayes classifiers have worked quite well in many real-world situations, famously **document classification** and **spam filtering**. They require a small amount of training data to estimate the necessary parameters.\n",
    "\n",
    "\n",
    "### ** What are the important Naive Nayes Distributions? **\n",
    "As discussed above, The different naive Bayes classifiers differ mainly by the assumptions they make regarding the distribution of $P(x_i \\mid y)$. These distributions are  \n",
    "\n",
    "#### **GaussianNB**\n",
    "\n",
    "It implements the Gaussian Naive Bayes algorithm for classification. The likelihood of the features is assumed to be Gaussian:\n",
    "\n",
    "$$ P \\left(x_i \\mid y\\right)=\\frac{1}{\\sqrt{2 \\pi \\sigma_{y_i}^2}} \\exp\\left(-\\frac{\\left(x_i - \\mu_y \\right)^2}{2 \\sigma_y^2}\\right) $$\n",
    "\n",
    "\n",
    "#### **Multinomial Naive Bayes**\n",
    "\n",
    "\n",
    "** Multinomial distribution ** is a generalization of the binomial distribution. For n independent trials each of which leads to a success for exactly one of k categories, with each category having a given fixed success probability, the multinomial distribution gives the probability of any particular combination of numbers of successes for the various categories.\n",
    "\n",
    "The ** binomial distribution ** is the probability distribution of the number of successes for one of just two categories in n independent Bernoulli trials, with the same probability of success on each trial. In a multinomial distribution, the analog of the Bernoulli distribution is the **categorical distribution**, where each trial results in exactly one of some fixed finite number k possible outcomes, with probabilities $p_1, ..., p_k$ (so that $pi ≥ 0$ for $i = 1, ..., k$ and $\\sum_{i=1}^k p_i = 1$, and there are n independent trials. Then if the random variables $X_i$ indicate the number of times outcome number $i$ is observed over the $n$ trials, the vector $X = (X_1, ..., X_k)$ follows a multinomial distribution with parameters $n$ and $p$, where $p = (p_1, ..., p_k)$. While the trials are independent, their outcomes $X$ are dependent because they must be summed to $n$.\n",
    "\n",
    "In [another words](http://www.stat.yale.edu/Courses/1997-98/101/binom.htm), the **binomial distribution** describes the behavior of a count variable X if the following conditions apply:\n",
    " 1. The number of observations n is fixed\n",
    " 2. Each observation is independent.\n",
    " 3. Each observation represents one of two outcomes (\"success\" or \"failure\").\n",
    " 4. The probability of \"success\" p is the same for each outcome.\n",
    "If these conditions are met, then X has a binomial distribution with parameters n and p, abbreviated $B(n,p)$.\n",
    "The probability that a random variable X with binomial distribution B(n,p) is equal to the value k, where k = 0, 1,....,n , is given by,\n",
    "\n",
    "\n",
    "$$ P(X=k) = \\binom{n}{k} p^k (1-p)^{n-k} $$\n",
    "\n",
    "\n",
    "** Categorial distribution** is a probability distribution that describes the possible results of a random event that can take on one of K possible outcomes, with the probability of each outcome separately specified. \n",
    "\n",
    "\n",
    "**[MultinomialNB](http://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes). ** implements the naive Bayes algorithm for multinomially distributed data, and is one of the two classic naive Bayes variants used in text classification (where the data are typically represented as word vector counts, although tf-idf vectors are also known to work well in practice). The distribution is parametrized by vectors $\\theta_y = (\\theta_{y1},\\ldots,\\theta_{yn})$ for each class $y$, where $n$ is the number of features (in text classification, the size of the vocabulary) and $\\theta_{yi}$ is the probability $P(x_i \\mid y)$ of feature $i$ appearing in a sample belonging to class $y$.\n",
    "\n",
    "The parameters $\\theta_y$ is estimated by a smoothed version of maximum likelihood, i.e. relative frequency counting:\n",
    "\n",
    "$$\\hat{\\theta}_{yi} = \\frac{ N_{yi} + \\alpha}{N_y + \\alpha n}$$\n",
    "\n",
    "where \n",
    "\n",
    "$$N_{yi} = \\sum_{x \\in T} x_i $$\n",
    "\n",
    "is the number of times feature i appears in a sample of class $y$ in the training set $T$, and \n",
    "\n",
    "$$N_{y} = \\sum_{i=1}^{|T|} N_{yi}$$ \n",
    "\n",
    "is the total count of all features for class $y$.\n",
    "The smoothing priors $\\alpha \\ge 0$ accounts for features not present in the learning samples and prevents zero probabilities in further computations. Setting $\\alpha = 1$ is called **Laplace smoothing**, while $\\alpha < 1$ is called **Lidstone smoothing**.\n",
    "\n",
    "\n",
    "#### **Bernoulli Naive Bayes**\n",
    "\n",
    "The probability distribution of a random variable which takes the value 1 with success probability of p and the value 0 with failure probability of q=1-p. It can be used to represent a coin toss where 1 and 0 would represent \"head\" and \"tail\" (or vice versa), respectively. The Bernoulli distribution is a special case of the **two-point distribution**, for which the two possible outcomes need not be 0 and 1. It is also a special case of the **binomial distribution**; *the Bernoulli distribution is a binomial distribution where n=1.*\n",
    "\n",
    "The decision rule for Bernoulli naive Bayes is based on\n",
    "\n",
    "\n",
    "$$P(x_i \\mid y) = P(i \\mid y) x_i + (1 - P(i \\mid y)) (1 - x_i)$$\n",
    "\n",
    "which differs from multinomial NB’s rule in that it explicitly penalizes the non-occurrence of a feature i that is an indicator for class y, where the multinomial variant would simply ignore a non-occurring feature. In the case of text classification, word occurrence vectors (rather than word count vectors) may be used to train and use this classifier. BernoulliNB might perform better on some datasets, especially those with shorter documents.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
