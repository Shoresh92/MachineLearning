{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Naive Bayes\n",
    "\n",
    "Among the lecture notes, videos and artcles I read, I found the [introduction to Naive Bayes](http://scikit-learn.org/stable/modules/naive_bayes.html) on scikit-learn website the most useful. to the page. For more extensive discussions, I suggest other books like [Baysian Data Analytics](http://www.amazon.com/Bayesian-Analysis-Chapman-Statistical-Science/dp/1439840954/) recommneded in a post on [Quora](https://www.quora.com/What-are-some-good-books-for-learning-probability-and-statistics).\n",
    "\n",
    "To understand it better, and in accordance to what suggested [here](http://machinelearningmastery.com/how-to-learn-a-machine-learning-algorithm/), I will try to answer some questions aiming at gaining a better grasp of the fundamental ideas behing Naive Bayes. \n",
    "\n",
    "** What are the fundamental concepts I need to know in order to better understand Naive Bayes? **\n",
    "\n",
    "* ** maximum a posteriori probability (MAP) ** \n",
    "\n",
    "In Bayesian statistics, a maximum a posteriori probability (MAP) estimate is a mode of the posterior distribution. The MAP can be used to obtain a point estimate of an unobserved quantity on the basis of empirical data. \n",
    "\n",
    "* ** Maximum Likelihood **\n",
    "\n",
    "In general, for a fixed set of data and underlying statistical model, the method of maximum likelihood selects the set of values of the model parameters that maximizes the likelihood function. Intuitively, this maximizes the \"agreement\" of the selected model with the observed data, and for discrete random variables it indeed maximizes *the probability of the observed data* under the resulting distribution. [wiki](https://en.wikipedia.org/wiki/Maximum_likelihood) *I am curious to learn more about the mathematical foundation behind it.* \n",
    "\n",
    "* ** posterier probability **\n",
    "\n",
    "The posterior probability is the probability of the parameters Y given the evidence X: p(Y|X).\n",
    "\n",
    "* **Maximum A Posteriori (MAP) estimation**\n",
    "[Source](https://www.cs.utah.edu/~suyash/Dissertation_html/node8.html)\n",
    "\n",
    "Sometimes we have a priori information about the physical process whose parameters we want to estimate. Such information can come either from the correct scientific knowledge of the physical process or from previous empirical evidence. We can encode such prior information in terms of a probability density function (PDF) on the parameter to be estimated. Essentially, we treat the parameter Y as the value of an random variable (RV). The associated probabilities P(Y) are called the prior probabilities. We refer to the inference based on such priors as **Bayesian inference**. Bayes' theorem shows the way for incorporating prior information in the estimation process: \n",
    "\n",
    "$$ P(Y|X) = P(X|Y)\\frac{P(Y)}{P(X)} $$\n",
    "\n",
    "The term on the left hand side is called the **posterior**. On the right hand side, the numerator is the product of the **likelihood term** and the **prior term**. The denominator serves as a **normalization term** so that the posterior PDF integrates to unity. Thus, Bayesian inference produces the maximum a posteriori (MAP) estimate \n",
    "$argmax_Y P(Y|X)= argmax_Y P(X|Y) P(Y)$\n",
    "\n",
    "Also, to better undertans the definition and concepts, reading [source](http://www.probabilitycourse.com/chapter9/9_1_2_MAP_estimation.php) is highly recommended. \n",
    "\n",
    "* **Point estimator**\n",
    "\n",
    "In statistics, point estimation involves the use of sample data to calculate a single value (known as a statistic) which is to serve as a \"best guess\" or \"best estimate\" of an unknown (fixed or random) population parameter.\n",
    "\n",
    "* **Regularization**: \n",
    "\n",
    "in mathematics and statistics and particularly in the fields of machine learning and inverse problems, refers to a process of introducing additional information in order to solve an ill-posed problem or to prevent overfitting. [Wiki](https://en.wikipedia.org/wiki/Regularization_(mathematics)\n",
    "\n",
    "\n",
    "In spite of their apparently over-simplified assumptions, naive Bayes classifiers have worked quite well in many real-world situations, famously **document classification** and **spam filtering**. They require a small amount of training data to estimate the necessary parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
