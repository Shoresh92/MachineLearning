{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Naive Bayes\n",
    "\n",
    "Among the lecture notes, videos and artcles I read, I found the [introduction to Naive Bayes](http://scikit-learn.org/stable/modules/naive_bayes.html) on scikit-learn website the most useful. to the page. For more extensive discussions, I suggest other books like [Baysian Data Analytics](http://www.amazon.com/Bayesian-Analysis-Chapman-Statistical-Science/dp/1439840954/) recommneded in a post on [Quora](https://www.quora.com/What-are-some-good-books-for-learning-probability-and-statistics).\n",
    "\n",
    "To understand it better, and in accordance to what suggested [here](http://machinelearningmastery.com/how-to-learn-a-machine-learning-algorithm/), I will try to answer some questions aiming at gaining a better grasp of the fundamental ideas behing Naive Bayes. \n",
    "\n",
    "** What are the fundamental concepts I need to know in order to better understand Naive Bayes? **\n",
    "\n",
    "* ** maximum a posteriori probability (MAP) ** \n",
    "\n",
    "In Bayesian statistics, a maximum a posteriori probability (MAP) estimate is a mode of the posterior distribution. The MAP can be used to obtain a point estimate of an unobserved quantity on the basis of empirical data. \n",
    "\n",
    "* ** Maximum Likelihood **\n",
    "\n",
    "In general, for a fixed set of data and underlying statistical model, the method of maximum likelihood selects the set of values of the model parameters that maximizes the likelihood function. Intuitively, this maximizes the \"agreement\" of the selected model with the observed data, and for discrete random variables it indeed maximizes *the probability of the observed data* under the resulting distribution. [wiki](https://en.wikipedia.org/wiki/Maximum_likelihood) *I am curious to learn more about the mathematical foundation behind it.* \n",
    "\n",
    "* ** posterier probability **\n",
    "\n",
    "The posterior probability is the probability of the parameters Y given the evidence X: p(Y|X).\n",
    "\n",
    "* **Maximum A Posteriori (MAP) estimation**\n",
    "[Source](https://www.cs.utah.edu/~suyash/Dissertation_html/node8.html)\n",
    "\n",
    "Sometimes we have a priori information about the physical process whose parameters we want to estimate. Such information can come either from the correct scientific knowledge of the physical process or from previous empirical evidence. We can encode such prior information in terms of a probability density function (PDF) on the parameter to be estimated. Essentially, we treat the parameter Y as the value of an random variable (RV). The associated probabilities P(Y) are called the prior probabilities. We refer to the inference based on such priors as **Bayesian inference**. Bayes' theorem shows the way for incorporating prior information in the estimation process: \n",
    "\n",
    "$$ P(Y|X) = P(X|Y) P(Y)/P(X) $$\n",
    "\n",
    "The term on the left hand side is called the **posterior**. On the right hand side, the numerator is the product of the **likelihood term** and the **prior term**. The denominator serves as a **normalization term** so that the posterior PDF integrates to unity. Thus, Bayesian inference produces the maximum a posteriori (MAP) estimate \n",
    "$argmax_Y P(Y|X)= argmax_Y P(X|Y) P(Y)$\n",
    "\n",
    "*second explanation*\n",
    "[source](http://www.probabilitycourse.com/chapter9/9_1_2_MAP_estimation.php)\n",
    "The posterior distribution, $f(Y|X)$, contains all the knowledge about the unknown quantity Y. Therefore, we can use the posterior distribution to find point or interval estimates of Y. One way to obtain a point estimate is to choose the value of Y that maximizes the posterior PDF (or PMF). This is called the maximum a posteriori (MAP) estimation.\n",
    "\n",
    "* **Point estimator**\n",
    "\n",
    "In statistics, point estimation involves the use of sample data to calculate a single value (known as a statistic) which is to serve as a \"best guess\" or \"best estimate\" of an unknown (fixed or random) population parameter.\n",
    "\n",
    "* **Regularization**: \n",
    "\n",
    "in mathematics and statistics and particularly in the fields of machine learning and inverse problems, refers to a process of introducing additional information in order to solve an ill-posed problem or to prevent overfitting. [Wiki](https://en.wikipedia.org/wiki/Regularization_(mathematics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
