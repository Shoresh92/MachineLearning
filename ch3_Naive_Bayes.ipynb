{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Naive Bayes\n",
    "\n",
    "Among the lecture notes, videos and artcles I read, I found the [introduction to Naive Bayes](http://scikit-learn.org/stable/modules/naive_bayes.html) on scikit-learn website the most useful. to the page. For more extensive discussions, I suggest other books like [Baysian Data Analytics](http://www.amazon.com/Bayesian-Analysis-Chapman-Statistical-Science/dp/1439840954/) recommneded in a post on [Quora](https://www.quora.com/What-are-some-good-books-for-learning-probability-and-statistics).\n",
    "\n",
    "To understand it better, and in accordance to what suggested [here](http://machinelearningmastery.com/how-to-learn-a-machine-learning-algorithm/), I will try to answer some questions aiming at gaining a better grasp of the fundamental ideas behing Naive Bayes. \n",
    "\n",
    "#### ** What are the fundamental concepts I need to know in order to better understand Naive Bayes? **\n",
    "\n",
    "* ** maximum a posteriori probability (MAP) ** \n",
    "\n",
    "In Bayesian statistics, a maximum a posteriori probability (MAP) estimate is a mode of the posterior distribution. The MAP can be used to obtain a point estimate of an unobserved quantity on the basis of empirical data. \n",
    "\n",
    "* ** Maximum Likelihood **\n",
    "\n",
    "In general, for a fixed set of data and underlying statistical model, the method of maximum likelihood selects the set of values of the model parameters that maximizes the likelihood function. Intuitively, this maximizes the \"agreement\" of the selected model with the observed data, and for discrete random variables it indeed maximizes *the probability of the observed data* under the resulting distribution. [wiki](https://en.wikipedia.org/wiki/Maximum_likelihood) *I am curious to learn more about the mathematical foundation behind it.* \n",
    "\n",
    "* ** posterier probability **\n",
    "\n",
    "The posterior probability is the probability of the parameters Y given the evidence X: p(Y|X).\n",
    "\n",
    "* **Maximum A Posteriori (MAP) estimation**\n",
    "[Source](https://www.cs.utah.edu/~suyash/Dissertation_html/node8.html)\n",
    "\n",
    "Sometimes we have a priori information about the physical process whose parameters we want to estimate. Such information can come either from the correct scientific knowledge of the physical process or from previous empirical evidence. We can encode such prior information in terms of a probability density function (PDF) on the parameter to be estimated. Essentially, we treat the parameter Y as the value of an random variable (RV). The associated probabilities P(Y) are called the prior probabilities. We refer to the inference based on such priors as **Bayesian inference**. Bayes' theorem shows [...] way for incorporating prior information in the estimation process: \n",
    "\n",
    "$$ P(Y|X) = P(X|Y)\\frac{P(Y)}{P(X)} $$\n",
    "\n",
    "The term on the left hand side is called the **posterior**. On the right hand side, the numerator is the product of the **likelihood term** and the **prior term**. The denominator serves as a **normalization term** so that the posterior PDF integrates to unity. Thus, Bayesian inference produces the maximum a posteriori (MAP) estimate \n",
    "$argmax_Y P(Y|X)= argmax_Y P(X|Y) P(Y)$\n",
    "\n",
    "*Also, to better understand the definition and concepts, reading [this link](http://www.probabilitycourse.com/chapter9/9_1_2_MAP_estimation.php) is highly recommended.*\n",
    "\n",
    "* **Point estimator**\n",
    "\n",
    "In statistics, point estimation involves the use of sample data to calculate a single value (known as a statistic) which is to serve as a \"best guess\" or \"best estimate\" of an unknown (fixed or random) population parameter.\n",
    "\n",
    "* **Regularization**: \n",
    "\n",
    "in mathematics and statistics and particularly in the fields of machine learning and inverse problems, refers to a process of introducing additional information in order to solve an ill-posed problem or to prevent overfitting. [Wiki](https://en.wikipedia.org/wiki/Regularization_(mathematics)\n",
    "\n",
    "In spite of their apparently over-simplified assumptions, naive Bayes classifiers have worked quite well in many real-world situations, famously **document classification** and **spam filtering**. They require a small amount of training data to estimate the necessary parameters.\n",
    "\n",
    "\n",
    "#### ** What are the important Naive Nayes Distributions? **\n",
    "As discussed above, The different naive Bayes classifiers differ mainly by the assumptions they make regarding the distribution of $P(x_i \\mid y)$. These distributions are  \n",
    "\n",
    "#### **GaussianNB**\n",
    "\n",
    "It is used in classification and it assumes that features follow a normal distribution. It implements the Gaussian Naive Bayes algorithm for classification. The likelihood of the features is assumed to be Gaussian:\n",
    "\n",
    "$$ P \\left(x_i \\mid y\\right)=\\frac{1}{\\sqrt{2 \\pi \\sigma_{y_i}^2}} \\exp\\left(-\\frac{\\left(x_i - \\mu_y \\right)^2}{2 \\sigma_y^2}\\right) $$\n",
    "\n",
    "\n",
    "#### **Multinomial Naive Bayes**\n",
    "\n",
    "It is used for discrete counts. For example, let’s say,  we have a text classification problem. Here we can consider bernoulli trials which is one step further and instead of “word occurring in the document”, we have “count how often word occurs in the document”, you can think of it as “number of times outcome number x_i is observed over the n trials”.\n",
    "\n",
    "** Multinomial distribution ** is a generalization of the binomial distribution. For n independent trials each of which leads to a success for exactly one of k categories, with each category having a given fixed success probability, the multinomial distribution gives the probability of any particular combination of numbers of successes for the various categories.\n",
    "\n",
    "The ** binomial distribution ** is the probability distribution of the number of successes for one of just two categories in n independent Bernoulli trials, with the same probability of success on each trial. In a multinomial distribution, the analog of the Bernoulli distribution is the **categorical distribution**, where each trial results in exactly one of some fixed finite number k possible outcomes, with probabilities $p_1, ..., p_k$ (so that $pi ≥ 0$ for $i = 1, ..., k$ and $\\sum_{i=1}^k p_i = 1$, and there are n independent trials. Then if the random variables $X_i$ indicate the number of times outcome number $i$ is observed over the $n$ trials, the vector $X = (X_1, ..., X_k)$ follows a multinomial distribution with parameters $n$ and $p$, where $p = (p_1, ..., p_k)$. While the trials are independent, their outcomes $X$ are dependent because they must be summed to $n$.\n",
    "\n",
    "In [another words](http://www.stat.yale.edu/Courses/1997-98/101/binom.htm), the **binomial distribution** describes the behavior of a count variable X if the following conditions apply:\n",
    " 1. The number of observations n is fixed\n",
    " 2. Each observation is independent.\n",
    " 3. Each observation represents one of two outcomes (\"success\" or \"failure\").\n",
    " 4. The probability of \"success\" p is the same for each outcome.\n",
    "If these conditions are met, then X has a binomial distribution with parameters n and p, abbreviated $B(n,p)$.\n",
    "The probability that a random variable X with binomial distribution B(n,p) is equal to the value k, where k = 0, 1,....,n , is given by,\n",
    "\n",
    "\n",
    "$$ P(X=k) = \\binom{n}{k} p^k (1-p)^{n-k} $$\n",
    "\n",
    "\n",
    "** Categorial distribution** is a probability distribution that describes the possible results of a random event that can take on one of K possible outcomes, with the probability of each outcome separately specified. \n",
    "\n",
    "\n",
    "**[MultinomialNB](http://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes).** \n",
    "\n",
    "The binomial model is useful if your feature vectors are binary (i.e. zeros and ones). One application would be text classification with ‘bag of words’ model where the 1s & 0s are “word occurs in the document” and “word does not occur in the document” respectively.\n",
    "\n",
    "This model implements the naive Bayes algorithm for multinomially distributed data, and is one of the two classic naive Bayes variants used in text classification (where the data are typically represented as word vector counts, although tf-idf vectors are also known to work well in practice). The distribution is parametrized by vectors $\\theta_y = (\\theta_{y1},\\ldots,\\theta_{yn})$ for each class $y$, where $n$ is the number of features (in text classification, the size of the vocabulary) and $\\theta_{yi}$ is the probability $P(x_i \\mid y)$ of feature $i$ appearing in a sample belonging to class $y$.\n",
    "\n",
    "The parameters $\\theta_y$ is estimated by a smoothed version of maximum likelihood, i.e. relative frequency counting:\n",
    "\n",
    "$$\\hat{\\theta}_{yi} = \\frac{ N_{yi} + \\alpha}{N_y + \\alpha n}$$\n",
    "\n",
    "where \n",
    "\n",
    "$$N_{yi} = \\sum_{x \\in T} x_i $$\n",
    "\n",
    "is the number of times feature i appears in a sample of class $y$ in the training set $T$, and \n",
    "\n",
    "$$N_{y} = \\sum_{i=1}^{|T|} N_{yi}$$ \n",
    "\n",
    "is the total count of all features for class $y$.\n",
    "The smoothing priors $\\alpha \\ge 0$ accounts for features not present in the learning samples and prevents zero probabilities in further computations. Setting $\\alpha = 1$ is called **Laplace smoothing**, while $\\alpha < 1$ is called **Lidstone smoothing**.\n",
    "\n",
    "\n",
    "#### **Bernoulli Naive Bayes**\n",
    "\n",
    "The probability distribution of a random variable which takes the value 1 with success probability of p and the value 0 with failure probability of q=1-p. It can be used to represent a coin toss where 1 and 0 would represent \"head\" and \"tail\" (or vice versa), respectively. The Bernoulli distribution is a special case of the **two-point distribution**, for which the two possible outcomes need not be 0 and 1. It is also a special case of the **binomial distribution**; *the Bernoulli distribution is a binomial distribution where n=1.*\n",
    "\n",
    "The decision rule for Bernoulli naive Bayes is based on\n",
    "\n",
    "\n",
    "$$P(x_i \\mid y) = P(i \\mid y) x_i + (1 - P(i \\mid y)) (1 - x_i)$$\n",
    "\n",
    "which differs from multinomial NB’s rule in that it explicitly penalizes the non-occurrence of a feature i that is an indicator for class y, where the multinomial variant would simply ignore a non-occurring feature. In the case of text classification, word occurrence vectors (rather than word count vectors) may be used to train and use this classifier. BernoulliNB might perform better on some datasets, especially those with shorter documents.\n",
    "\n",
    "\n",
    "\n",
    "### [6 Easy Steps to Learn Naive Bayes Algorithm](http://www.analyticsvidhya.com/blog/2015/09/naive-bayes-explained/)\n",
    "\n",
    "#### What is Naive Bayes algorithm?\n",
    "\n",
    "It is a classification technique based on Bayes’ Theorem with an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. For example, a fruit may be considered to be an apple if it is red, round, and about 3 inches in diameter. Even if these features depend on each other or upon the existence of the other features, all of these properties independently contribute to the probability that this fruit is an apple and that is why it is known as ‘Naive’.\n",
    "\n",
    "\n",
    "#### Applications? \n",
    "\n",
    "This algorithm is mostly used in text classification and with problems having multiple classes.\n",
    "\n",
    "* **Real time Prediction **: Naive Bayes is an eager learning classifier and it is sure fast. Thus, it could be used for making predictions in real time.\n",
    "\n",
    "* **Multi class Prediction **: This algorithm is also well known for multi class prediction feature. Here we can predict the probability of multiple classes of target variable.\n",
    "\n",
    "* **Text classification/ Spam Filtering/ Sentiment Analysis **: Naive Bayes classifiers mostly used in text classification (due to better result in multi class problems and independence rule) have higher success rate as compared to other algorithms. As a result, it is widely used in Spam filtering (identify spam e-mail) and Sentiment Analysis (in social media analysis, to identify positive and negative customer sentiments)\n",
    "\n",
    "* **Recommendation System**: Naive Bayes Classifier and Collaborative Filtering together builds a Recommendation System that uses machine learning and data mining techniques to filter unseen information and predict whether a user would like a given resource or not\n",
    "\n",
    "\n",
    "#### What are the Pros and Cons of Naive Bayes?\n",
    "\n",
    "Pros:\n",
    "\n",
    "* It is easy and fast to predict class of test data set. It also perform well in multi class prediction\n",
    "\n",
    "* When assumption of independence holds, a Naive Bayes classifier performs better compare to other models like logistic regression and you need less training data.\n",
    "\n",
    "* It perform well in case of categorical input variables compared to numerical variable(s). For numerical variable, normal distribution is assumed (bell curve, which is a strong assumption).\n",
    "\n",
    "Cons:\n",
    "\n",
    "* If categorical variable has a category (in test data set), which was not observed in training data set, then model will assign a 0 (zero) probability and will be unable to make a prediction. This is often known as ** Zero Frequency **. To solve this, we can use the ** smoothing technique **. One of the simplest smoothing techniques is called ** Laplace estimation**.\n",
    "\n",
    "* On the other side naive Bayes is also known as a bad estimator, so the probability outputs from predict_proba are not to be taken too seriously.\n",
    "\n",
    "* Another limitation of Naive Bayes is the assumption of independent predictors. In real life, it is almost impossible that we get a set of predictors which are completely independent.\n",
    "\n",
    "\n",
    "** Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 4]\n"
     ]
    }
   ],
   "source": [
    "#Import Library of Gaussian Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "\n",
    "#assigning predictor and target variables\n",
    "x= np.array([[-3,7],[1,5], [1,2], [-2,0], [2,3], [-4,0], [-1,1], [1,1], [-2,2], [2,7], [-4,1], [-2,7]])\n",
    "Y = np.array([3, 3, 3, 3, 4, 3, 3, 4, 3, 4, 4, 4])\n",
    "#Create a Gaussian Classifier\n",
    "model = GaussianNB()\n",
    "\n",
    "# Train the model using the training sets \n",
    "model.fit(x, Y)\n",
    "\n",
    "#Predict Output \n",
    "predicted= model.predict([[1,2],[3,4]])\n",
    "print predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A great article: [Text Classification using Naive Bayes](http://www.inf.ed.ac.uk/teaching/courses/inf2b/learnnotes/inf2b-learn-note07-2up.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
