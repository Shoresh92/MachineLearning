{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Naive Bayes\n",
    "\n",
    "Among the lecture notes, videos and artcles I read, I found the [introduction to Naive Bayes](http://scikit-learn.org/stable/modules/naive_bayes.html) on scikit-learn website the most useful. to the page. For more extensive discussions, I suggest other books like [Baysian Data Analytics](http://www.amazon.com/Bayesian-Analysis-Chapman-Statistical-Science/dp/1439840954/) recommneded in a post on [Quora](https://www.quora.com/What-are-some-good-books-for-learning-probability-and-statistics).\n",
    "\n",
    "To understand it better, and in accordance to what suggested [here](http://machinelearningmastery.com/how-to-learn-a-machine-learning-algorithm/), I will try to answer some questions aiming at gaining a better grasp of the fundamental ideas behing Naive Bayes. \n",
    "\n",
    "#### ** What are the fundamental concepts I need to know in order to better understand Naive Bayes? **\n",
    "\n",
    "* ** maximum a posteriori probability (MAP) ** \n",
    "\n",
    "In Bayesian statistics, a maximum a posteriori probability (MAP) estimate is a mode of the posterior distribution. The MAP can be used to obtain a point estimate of an unobserved quantity on the basis of empirical data. \n",
    "\n",
    "* ** Maximum Likelihood **\n",
    "\n",
    "In general, for a fixed set of data and underlying statistical model, the method of maximum likelihood selects the set of values of the model parameters that maximizes the likelihood function. Intuitively, this maximizes the \"agreement\" of the selected model with the observed data, and for discrete random variables it indeed maximizes *the probability of the observed data* under the resulting distribution. [wiki](https://en.wikipedia.org/wiki/Maximum_likelihood) *I am curious to learn more about the mathematical foundation behind it.* \n",
    "\n",
    "* ** posterier probability **\n",
    "\n",
    "The posterior probability is the probability of the parameters Y given the evidence X: p(Y|X).\n",
    "\n",
    "* **Maximum A Posteriori (MAP) estimation**\n",
    "[Source](https://www.cs.utah.edu/~suyash/Dissertation_html/node8.html)\n",
    "\n",
    "Sometimes we have a priori information about the physical process whose parameters we want to estimate. Such information can come either from the correct scientific knowledge of the physical process or from previous empirical evidence. We can encode such prior information in terms of a probability density function (PDF) on the parameter to be estimated. Essentially, we treat the parameter Y as the value of an random variable (RV). The associated probabilities P(Y) are called the prior probabilities. We refer to the inference based on such priors as **Bayesian inference**. Bayes' theorem shows [...] way for incorporating prior information in the estimation process: \n",
    "\n",
    "$$ P(Y|X) = P(X|Y)\\frac{P(Y)}{P(X)} $$\n",
    "\n",
    "The term on the left hand side is called the **posterior**. On the right hand side, the numerator is the product of the **likelihood term** and the **prior term**. The denominator serves as a **normalization term** so that the posterior PDF integrates to unity. Thus, Bayesian inference produces the maximum a posteriori (MAP) estimate \n",
    "$argmax_Y P(Y|X)= argmax_Y P(X|Y) P(Y)$\n",
    "\n",
    "*Also, to better understand the definition and concepts, reading [this link](http://www.probabilitycourse.com/chapter9/9_1_2_MAP_estimation.php) is highly recommended.*\n",
    "\n",
    "* **Point estimator**\n",
    "\n",
    "In statistics, point estimation involves the use of sample data to calculate a single value (known as a statistic) which is to serve as a \"best guess\" or \"best estimate\" of an unknown (fixed or random) population parameter.\n",
    "\n",
    "* **Regularization**: \n",
    "\n",
    "in mathematics and statistics and particularly in the fields of machine learning and inverse problems, refers to a process of introducing additional information in order to solve an ill-posed problem or to prevent overfitting. [Wiki](https://en.wikipedia.org/wiki/Regularization_(mathematics)\n",
    "\n",
    "In spite of their apparently over-simplified assumptions, naive Bayes classifiers have worked quite well in many real-world situations, famously **document classification** and **spam filtering**. They require a small amount of training data to estimate the necessary parameters.\n",
    "\n",
    "\n",
    "#### ** What are the impmortant Naive Nayes Distributions? **\n",
    "As discussed above, The different naive Bayes classifiers differ mainly by the assumptions they make regarding the distribution of $P(x_i \\mid y)$. These distributions are  \n",
    "\n",
    "1. **GaussianNB**\n",
    "\n",
    "It implements the Gaussian Naive Bayes algorithm for classification. The likelihood of the features is assumed to be Gaussian:\n",
    "\n",
    "$$ P \\left(x_i \\mid y\\right)=\\frac{1}{\\sqrt{2 \\pi \\sigma_{y_i}^2}} \\exp\\left(-\\frac{\\left(x_i - \\mu_y \\right)^2}{2 \\sigma_y^2}\\right) $$\n",
    "\n",
    "\n",
    "2. **Multinomial Naive Bayes**\n",
    "\n",
    "\n",
    "* What is multinomial distribution?\n",
    "\n",
    "It is a generalization of the binomial distribution. For n independent trials each of which leads to a success for exactly one of k categories, with each category having a given fixed success probability, the multinomial distribution gives the probability of any particular combination of numbers of successes for the various categories.\n",
    "\n",
    "The binomial distribution is the probability distribution of the number of successes for one of just two categories in n independent Bernoulli trials, with the same probability of success on each trial. In a multinomial distribution, the analog of the Bernoulli distribution is the **categorical distribution**, where each trial results in exactly one of some fixed finite number k possible outcomes, with probabilities $p_1, ..., p_k$ (so that $pi â‰¥ 0$ for $i = 1, ..., k$ and $\\sum_{i=1}^k p_i = 1$, and there are n independent trials. Then if the random variables Xi indicate the number of times outcome number i is observed over the n trials, the vector $X = (X_1, ..., X_k)$ follows a multinomial distribution with parameters n and p, where $p = (p_1, ..., p_k)$. While the trials are independent, their outcomes X are dependent because they must be summed to n.\n",
    "\n",
    "* what is categorial distribution? \n",
    "\n",
    "It is a probability distribution that describes the possible results of a random event that can take on one of K possible outcomes, with the probability of each outcome separately specified. \n",
    "\n",
    "3. **Bernoulli Naive Bayes**\n",
    "\n",
    "In probability theory and statistics, the Bernoulli distribution, is the probability distribution of a random variable which takes the value 1 with success probability of p and the value 0 with failure probability of q=1-p. It can be used to represent a coin toss where 1 and 0 would represent \"head\" and \"tail\" (or vice versa), respectively. The Bernoulli distribution is a special case of the **two-point distribution**, for which the two possible outcomes need not be 0 and 1. It is also a special case of the **binomial distribution**; the Bernoulli distribution is a binomial distribution where n=1\n",
    "\n",
    "In [another words](http://www.stat.yale.edu/Courses/1997-98/101/binom.htm), the binomial distribution describes the behavior of a count variable X if the following conditions apply:\n",
    " 1. The number of observations n is fixed\n",
    " 2. Each observation is independent.\n",
    " 3. Each observation represents one of two outcomes (\"success\" or \"failure\").\n",
    " 4. The probability of \"success\" p is the same for each outcome.\n",
    "If these conditions are met, then X has a binomial distribution with parameters n and p, abbreviated $B(n,p)$.\n",
    "The probability that a random variable X with binomial distribution B(n,p) is equal to the value k, where k = 0, 1,....,n , is given by,\n",
    "\n",
    "$$ P(X=k) = \\binom{n}{k} p^k (1-p)^{n-k} $$\n",
    " \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
