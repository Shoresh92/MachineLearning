{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Notes\n",
    "\n",
    "\n",
    "## 1. Introduction to Machine Learning\n",
    "\n",
    "\n",
    "### 1.1 Intro to Machine Learning\n",
    "\n",
    "** Model **: a model is a relationship that exist between different variables. \n",
    "\n",
    "** Machine Learning **: we use it to refer to creating and using models that are leaned from data which in other content might be called predictive modeling or data mining. \n",
    "\n",
    "** Features **: are whatever inputs we provide to our model. We choose features based on experience and domain expertise \n",
    "Supervised vs. Unsupervised Learning\n",
    "\n",
    "** Supervised learning **: there is a set of data labelled with the correct answers to learn from ",
    "In supervised learning, we relate response to predictions (prediction), find a relation between the response and the prediction (inference)\n",
    "\n",
    "**Unsupervised learning**: in which there are no such labels as supervised learning.  ",
    "In unsupervised learning, there is no associated response for the observation: no response to predict and consequently no regression model. \n",
    "Therefore, the analysis that can be done for this case is to find relations between variables or observables, like clustering. \n",
    "\n",
    "**overfitting**: a model the performs well on the training set and does a poor job on the new data. Low bias and high variance leads to overfitting. \n",
    "\n",
    "**under-fitting**: does a poor job on both trading and new data. High bias and low variance corresponds to under-fitting.      \n",
    "\n",
    "**Bias and Variance**\n",
    "**Bias**: the difference in prediction that arise due to use of different models. To fix high bias one should add features.\n",
    "\n",
    "** Variance **: \n",
    "the difference in prediction that arise due to use of different training sets. \n",
    "To improve the model prediction by hight variance we should remove features. \n",
    "\n",
    "\n",
    "### 1.2. How to choose the right model?\n",
    "\n",
    "How to choose a model that performs best on the test set? In this situation we should split the data into three parts: a training set to build the models, a validation set for choosing among trained models, and a test set for judging the final model.  \n",
    "Pretty much always we extract features from our data that far into one of these three categories:  ",
    "i) yes or no which we can encode as 0 or 1 —> Naive Bayes Classifier  ",
    "ii) we require numerical features —> Regression Models ",
    "iii) we have a choice from discrete set of options —> Decision Trees that can deal with numeric or categorical data\n",
    "\n",
    "Bias and Variance\n",
    "Bias: the difference in prediction that arise due to use of different models. To fix high bias one should add features. \n",
    "Variance: \n",
    "the difference in prediction that arise due to use of different training sets. \n",
    "To improve the model prediction by hight variance we should remove features. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 2. SciKit Learn\n",
    "\n",
    "### 2.1 Starting with SciKit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loading a data base\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "type(iris)\n",
    "iris.data\n",
    "iris.feature_names\n",
    "iris.target #target is what we are going to predict\n",
    "iris.target_names\n",
    "\n",
    "iris.data.shape # to get the dimension of the features and target\n",
    "iris.target.shape\n",
    "\n",
    "X = iris.data  # Storing feature\n",
    "y = iris.target # Storing response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.2 K nearest neighbor \n",
    "\n",
    "**Source**: Data Science from Scratch & http://scikit-learn.org/stable/modules/neighbors.html\n",
    "\n",
    "\n",
    "#### Introduction\n",
    "\n",
    "1. As a simplest predictive model, makes no mathematical assumption, and does not require any sort of heavy machinary. The only requirements are i) some notion of distance, and ii) an assumption that points that are close to one another are similar \n",
    "\n",
    "2. The K nearest neighbor predictive model does not help to understand the drivers of whatever phenomena we are looking at. \n",
    "\n",
    "3. **curse of dimensionality**: points in high-dimensional spaces tend not to be close to one another at all. \n",
    "\n",
    "4. It's great for many applications, with personalization tasks being among the most common. To make a personalized offer to one customer, you might employ KNN to find similar customers and base your offer on their purchase behaviors. KNN has also been applied to medical diagnosis and credit scoring.\n",
    "\n",
    "5. Scikit expects X (feature matrix) and y (response vector) to be Numpy arrays. \n",
    "\n",
    "\n",
    "#### 2.2.1 What is k-Nearest Neighbors?\n",
    "The model for kNN is the entire training dataset. When a prediction is required for a unseen data instance, the kNN algorithm will search through the training dataset for the k-most similar instances. The prediction attribute of the most similar instances is summarized and returned as the prediction for the unseen instance.\n",
    "\n",
    "The similarity measure is dependent on the type of data. For real-valued data, the Euclidean distance can be used. Other other types of data such as categorical or binary data, Hamming distance can be used.\n",
    "In the case of regression problems, the average of the predicted attribute may be returned. In the case of classification, the most prevalent class may be returned.\n",
    "\n",
    "\n",
    "#### 2.2.2 Nearest Neighbor Algorithms\n",
    "1. **Brute Force**: computation of distances between all pairs of points in the dataset\n",
    "\n",
    "2. K-D Tree: the basic idea to make more efficient algorithm is that if point A is very distant from point B, and point B is very close to point C, then we know that points A and C are very distant, without having to explicitly calculate their distance.  ",
    "(https://www.youtube.com/watch?v=TLxWtXEbtFE)\n",
    "\n",
    "3. ** Ball Tree **: To address the inefficiencies of KD Trees in higher dimensions, the ball tree data structure was developed. Where KD trees partition data along Cartesian axes, ball trees partition data in a series of nesting hyper-spheres. This makes tree construction more costly than that of the KD tree, but results in a data structure which can be very efficient on highly-structured data, even in very high dimensions. Because of the spherical geometry of the ball tree nodes, it can out-perform a KD-tree in high dimensions, though the actual performance is highly dependent on the structure of the training data.\n",
    "\n",
    "4. The choice of neighbors search algorithm is controlled through the keyword 'algorithm', which must be one of ['auto', 'ball_tree', 'kd_tree', ‘brute’].\n",
    "\n",
    "\n",
    "#### 2.2.3 Scikit Nearest Neighbors\n",
    "\n",
    "Nearest Neighbors Classification\n",
    "Neighbors-based classification does not attempt to construct a general internal model, but simply stores instances of the training data. Classification is computed from a simple majority vote of the nearest neighbors of each point: a query point is assigned the data class which has the most representatives within the nearest neighbors of the point.\n",
    "scikit-learn implements two different nearest neighbors classifiers: KNeighborsClassifier and RadiusNeighborsClassifier. When the data is not uniformly sampled, the latter can be a better choice. For high-dimensional parameter spaces, this method becomes less effective due to the so-called “curse of dimensionality”.\n",
    "Default is using uniform weights. To weight the neighbors that may contribute more to the fit, one can change weights = ‘uniform' to weights = ‘distance’ or a user-defined function. \n",
    "\n",
    "An Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "X = iris.data \n",
    "y = iris.target\n",
    "\n",
    "X.shape \n",
    "y.shape\n",
    "\n",
    "from sklearn.neighbors import KNeighbrosClassifier \n",
    "knn = KNeighborClassifier(n_neighbors = 5)\n",
    "# to see all the default values print knn\n",
    "knn.predict([3,5,4,2])\n",
    "# or \n",
    "X_New = [[3,5,4,2], [5,4,3,2]] \n",
    "# and then \n",
    "knn.predict(X_New)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Another way:  \n",
    "knn.fit(X,y)\n",
    "knn.predic(X_New)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### 2.2.4 Nearest Neighbors Regression\n",
    "\n",
    "1. Neighbors-based regression can be used where the data labels are continuous variables. The label assigned to a query point is computed based the mean of the labels of its nearest neighbors.\n",
    "\n",
    "2. **KNeighborsRegressor** and **RadiusNeighborsRegressor**\n",
    "\n",
    "\n",
    "#### 2.2.5 Comparing different Models\n",
    "\n",
    "The video below is really useful as it employs scikit tools to do the whole process. https://www.youtube.com/watch?v=0pP4EwWJgIU&index=5&list=PL5-da3qGB5ICeMbQuqbbCOQWcS6OYBr5A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using Regression method\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics # for classification accuracy\n",
    "X.shape\n",
    "y.shape \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
    "X_train.shape # To test the new trading and test datatbase, \n",
    "y_train.shape \n",
    "\n",
    "logreg = LogistinRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "print metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using k nearest neighbor method\n",
    "knn = kNeighborsClassifier(n_neighbors = 5)\n",
    "kn.fit(X_train, y_train)\n",
    "y_pred = knn,predict(X_test)\n",
    "print metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
